# ğŸ“Š IAB Verticalizer
Production-grade classification system for website content into IAB categories with geo-specific Premiumness Scores

See also:
- spec.md (Project Specification: scope, requirements, acceptance)
- CONTRIBUTING.md (Contributor workflow, QA gates)

---

## ğŸ“Œ Overview

IAB Verticalizer is a production-ready ML system for:
- Classifying websites into IAB categories (Tierâ€‘1; optionally Tierâ€‘2).
- Assigning geo-specific Premiumness Scores (1â€“10) per category.
- Operating on labeled datasets, unlabeled datasets (via crawling, embedding, inference), or both in hybrid loops.
- Combining crawled website content + Gemini embeddings + Keras classifier with calibration.
- Achieving production-level robustness, modularity, caching, and reproducibility.

Inspired by â€œKaggle Day 2 â€” Classifying embeddings with Keras,â€ extended for multiâ€‘label, multiâ€‘geo, production deployments.

---

## ğŸš€ Features

- Multiâ€‘label classification with Keras and semantic embeddings  
- Gemini API integration with caching, rate limiting, and dryâ€‘run modes  
- Website crawling with robots.txt compliance  
- Geoâ€‘specific Premiumness Scores (1â€“10)  
- Probability calibration (Isotonic Regression)  
- Modular pipeline: crawl, embed, train, infer, eval â€” each runnable independently  
- Fileâ€‘driven I/O with Postgres & optional S3  
- Logging, retries & idempotent reruns for production

---

## ğŸ— Architecture & Workflow

```mermaid
flowchart LR
subgraph Input
A[CSV: websites, labels]
end

subgraph Apps
CRAWL[Crawlerapps/crawler]
EMBED[Embedderapps/embedder]
TRAIN[Trainerapps/trainer]
INFER[Inferapps/infer]
EVAL[Evaluateapps/evaluate]
end

subgraph Pipeline
COMMON["prepare_embeddings_for_df()pipeline/common.py"]
NODES["nodes.py(train, infer, eval)"]
end

subgraph Storage
PG[(Postgres)]
S3[(Object Store)]
end

subgraph Artifacts
MODELS[(Model + Calib Registry)]
PRED[Predictions JSONL]
REPORT[Eval JSON]
end

%% Input feeds site lists/labels to apps
A --> CRAWL
A --> EMBED
A --> TRAIN
A --> INFER
A --> EVAL

%% Crawler persists content
CRAWL --> PG
CRAWL --> S3

%% Embedder reads latest crawls and persists embeddings metadata
PG --> EMBED
S3 --> EMBED
EMBED --> PG

%% Shared prep pulls text/embeddings via storage
PG --> COMMON
S3 --> COMMON
COMMON --> NODES

%% Training uses common prep and writes models
TRAIN --> NODES
NODES --> PG
NODES --> MODELS

%% Inference uses models + common prep and writes predictions
MODELS --> INFER
INFER --> NODES
NODES --> PRED

%% Evaluation reads predictions (or model + gold) and writes report
PRED --> EVAL
MODELS --> EVAL
EVAL --> REPORT
```

---

## ğŸ“‚ Project Structure

```
verticalizer/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ spec.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/verticalizer/
â”‚   â”œâ”€â”€ cli.py                     # top-level CLI
â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â”œâ”€â”€ crawler/{cli.py, service.py, README.md}
â”‚   â”‚   â”œâ”€â”€ embedder/{cli.py, service.py, README.md}
â”‚   â”‚   â”œâ”€â”€ trainer/{cli.py, service.py, README.md}
â”‚   â”‚   â”œâ”€â”€ infer/{cli.py, service.py, README.md}
â”‚   â”‚   â””â”€â”€ evaluate/{cli.py, service.py, README.md}
â”‚   â”œâ”€â”€ crawl/                     # fetch, parse, robots
â”‚   â”œâ”€â”€ embeddings/                # gemini client + cache
â”‚   â”œâ”€â”€ models/                    # keras, calibration, persistence, registry
â”‚   â”œâ”€â”€ pipeline/                  # common helpers, nodes (train/infer/eval), io
â”‚   â”œâ”€â”€ storage/                   # postgres + s3 clients and repositories
â”‚   â””â”€â”€ utils/                     # logging, taxonomy, metrics, seed
â””â”€â”€ tests/
```

---

## ğŸ“Š Data Contracts

### Labeled CSV
| Column               | Type        | Required | Description                                |
|----------------------|-------------|----------|--------------------------------------------|
| website              | string      | âœ…        | Domain or URL                              |
| iab_labels           | list/string | âŒ        | JSON list or commaâ€‘separated IAB IDs       |
| premiumness_labels   | JSON        | âŒ        | Dict {IAB_ID: 1â€“10}                        |
| content_text         | string      | âŒ        | Optional preâ€‘fetched content               |

### Unlabeled CSV
| Column         | Type   | Required | Description   |
|----------------|--------|----------|---------------|
| website        | string | âœ…        | Domain or URL |
| content_text   | string | âŒ        | Optional      |

### Predictions JSONL (example)
```
{
"website": "cnn.com",
"geo": "US",
"categories": [
{ "id": "IAB12", "label": "News", "prob": 0.98, "score": 10 },
{ "id": "IAB14", "label": "Society", "prob": 0.76, "score": 8 },
{ "id": "IAB15", "label": "Science", "prob": 0.65, "score": 7 }
],
"generated_at": "2025-08-12T12:00:00Z"
}
```

---

## âš™ï¸ Installation

```
curl -sSL https://install.python-poetry.org | python3 -
poetry install
cp .env.example .env
```

### Core Env Vars
```
GEMINI_API_KEY=your_key
HTTP_USER_AGENT=Mozilla/5.0 (compatible; IABVerticalizer/1.0)
```

### Optional / Recommended
```
DATABASE_URL=postgresql+psycopg2://user:pass@host:5432/dbname
S3_ENDPOINT=http://localhost:9000
S3_BUCKET=verticalizer
S3_ACCESS_KEY=...
S3_SECRET_KEY=...

GEMINI_EMB_MODEL=models/text-embedding-004
GEMINI_EMB_DIM=768
GEMINI_TASK_TYPE=classification
GEMINI_EMB_DRYRUN=0
GEMINI_EMB_MAX_CALLS=0
GEMINI_EMB_RATE_LIMIT=0
```

---

## ğŸ“œ Commands

All commands share the topâ€“level entrypoint:
```
poetry run verticalizer  [options...]
```

### 1) Crawl
Fetch & parse website content, respecting robots.txt; persist to Postgres/S3.
```
poetry run verticalizer crawl \
--in PATH_TO_CSV \
[--store-html] \
[--geo GEO_CODE] \
[--batch-size N] \
[--max-sites N]
```

### 2) Embed
Generate/reuse cached embeddings; respects rate limits and cost controls.
```
poetry run verticalizer embed \
--in PATH_TO_CSV \
[--model MODEL_NAME] \
[--store-to-s3]
```

### 3) Train
Train two-head Keras model (labels + premiumness) & calibrate.
```
poetry run verticalizer train \
--geo GEO_CODE \
--in PATH_TO_LABELED_CSV \
--version VERSION_TAG \
--out-base MODELS_DIR
```

### 4) Infer
Run inference, preparing embeddings as needed, output JSONL.
```
poetry run verticalizer infer \
--geo GEO_CODE \
--in PATH_TO_CSV \
--model PATH_TO_MODEL \
--calib PATH_TO_CALIB \
--out OUTPUT_JSONL \
[--topk N]
```

### 5) Eval
Compare predictions JSONL vs. gold JSON to generate a report.
```
poetry run verticalizer eval \
--pred PREDICTIONS_JSONL \
--gold GOLD_LABELS_JSON \
--out OUTPUT_REPORT_JSON
```

### 6) runâ€‘pipeline (optional)
Chain all stages: crawl â†’ embed â†’ train â†’ infer â†’ eval.
```
poetry run verticalizer run-pipeline \
--geo GEO_CODE \
--in PATH_TO_LABELED_CSV \
--version VERSION_TAG \
--out-base MODELS_DIR
```

---

## ğŸ“ˆ Performance Tips

- Balance labeled data per geo for higher macroâ€‘F1.
- Enable embedding cache to reduce Gemini API cost/time.
- Use rate limits and max-calls env vars for budget control.
- Prefer batch embedding for efficiency and cache locality.

---

## ğŸ”’ Security & Compliance

- robots.txt respected by crawler.
- No PII stored.
- API keys in `.env` only.
- Optional domain allowlist for crawl compliance.

---

## ğŸ§ª Testing & Quality

```
poetry run pytest
poetry run ruff check src --fix
```

For contributor workflow, Makefile targets, and PR process, see CONTRIBUTING.md.

---

## ğŸ“š Specification

For scope, requirements, acceptance criteria, risks, persistence contracts, and roadmap, see spec.md.

---

## ğŸ· License

Proprietary â€“ Internal use only

---